{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU를 사용합니다. GeForce GTX 1050 Ti with Max-Q Design\n",
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "from winterschool_trainer import WinterSchool_BERT\n",
    "t = WinterSchool_BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kcbert                                    beomi@github 님이 만드신 KcBERT 학습데이터\nkorean_chatbot_data                       songys@github 님이 만드신 챗봇 문답 데이터\nkorean_hate_speech                        {inmoonlight,warnikchow,beomi}@github 님이 만드신 혐오댓글데이터\nkorean_parallel_koen_news                 jungyeul@github 님이 만드신 병렬 말뭉치\nkorean_petitions                          lovit@github 님이 만드신 2017.08 ~ 2019.03 청와대 청원데이터\nkornli                                    KakaoBrain 에서 제공하는 Natural Language Inference (NLI) 데이터\nkorsts                                    KakaoBrain 에서 제공하는 Semantic Textual Similarity (STS) 데이터\nkowikitext                                lovit@github 님이 만드신 wikitext 형식의 한국어 위키피디아 데이터\nnamuwikitext                              lovit@github 님이 만드신 wikitext 형식의 나무위키 데이터\nnaver_changwon_ner                        네이버 + 창원대 NER shared task data\nnsmc                                      e9t@github 님이 만드신 Naver sentiment movie corpus v1.0\nquestion_pair                             songys@github 님이 만드신 질문쌍(Paired Question v.2)\nmodu_news                                 국립국어원에서 만든 모두의 말뭉치: 뉴스 말뭉치\nmodu_messenger                            국립국어원에서 만든 모두의 말뭉치: 메신저 말뭉치\nmodu_mp                                   국립국어원에서 만든 모두의 말뭉치: 형태 분석 말뭉치\nmodu_ne                                   국립국어원에서 만든 모두의 말뭉치: 개체명 분석 말뭉치\nmodu_spoken                               국립국어원에서 만든 모두의 말뭉치: 구어 말뭉치\nmodu_web                                  국립국어원에서 만든 모두의 말뭉치: 웹 말뭉치\nmodu_written                              국립국어원에서 만든 모두의 말뭉치: 문어 말뭉치\nopen_subtitles                            Open parallel corpus (OPUS) 에서 제공하는 영화 자막 번역 병렬 말뭉치\naihub_translation                         AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어 + 대화 + 뉴스 + 한국문화 + 조례 + 지자체웹사이트)\naihub_spoken_translation                  AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어)\naihub_conversation_translation            AI Hub 에서 제공하는 번역용 병렬 말뭉치 (대화)\naihub_news_translation                    AI Hub 에서 제공하는 번역용 병렬 말뭉치 (뉴스)\naihub_korean_culture_translation          AI Hub 에서 제공하는 번역용 병렬 말뭉치 (한국문화)\naihub_decree_translation                  AI Hub 에서 제공하는 번역용 병렬 말뭉치 (조례)\naihub_government_website_translation      AI Hub 에서 제공하는 번역용 병렬 말뭉치 (지자체웹사이트)\n"
     ]
    }
   ],
   "source": [
    "t.get_corpus_specifications()"
   ]
  },
  {
   "source": [
    "## Wordpiece Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['전세계', '##에서', '코', '##로', '##나', '##19', '##보다', '훨씬', '전파', '##력이', '강한', '변', '##이', '바이러스', '확산', '##이', '끊', '##이지', '않는', '가운데', '마스크', '##를', '두', '##장', '##씩', '겹', '##쳐', '쓰', '##라는', '전문가들', '##의', '권고', '##가', '나온다', '##고', 'CNN', '##방송', '등이', '28', '##일', '(', '현지시간', ')', '보도했다', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = t.tokenizer.tokenize(\"전세계에서 코로나19보다 훨씬 전파력이 강한 변이 바이러스 확산이 끊이지 않는 가운데 마스크를 두장씩 겹쳐 쓰라는 전문가들의 권고가 나온다고 CNN방송 등이 28일(현지시간) 보도했다.\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[    2,  5708,    23,   731,    19,    64,  2174,   230,  3875,  4749,\n          2112,  2421,   813,     9,  5780,  3297,     9,  2545,  4256,  1030,\n           565,  7653,    14,   144,   100,  1259, 12137,  1116,   918,   612,\n          2595,     7,  6305,    16,  1574,    26,  6567,  2187,   621,   652,\n            22,    17,  4091,    18,  1546,     5,     3]])\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "t.encode([tokenized], _tqdm=False)\n",
    "print(t.input_ids)\n",
    "print(t.attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT: FineTuning -> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU를 사용합니다. GeForce GTX 1050 Ti with Max-Q Design\n",
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of the model checkpoint at /home/seongtae/SynologyDrive/SIRE/Projects/KR-BERT/KR-BERT/krbert_pytorch/pretrained/pytorch_model_char16424_ranked.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/seongtae/SynologyDrive/SIRE/Projects/KR-BERT/KR-BERT/krbert_pytorch/pretrained/pytorch_model_char16424_ranked.bin and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BERT 준비 완료\n"
     ]
    }
   ],
   "source": [
    "from winterschool_trainer import Sentiment_Analysis\n",
    "sent_analyzer = Sentiment_Analysis(train_ratio=0.9, batch_size=8, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/seongtae/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/seongtae/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "nsmc = sent_analyzer.build_corpus(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T06:06:23.367560Z",
     "start_time": "2021-01-28T06:06:23.277997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>텍스트</th>\n",
       "      <th>감정</th>\n",
       "      <th>라벨</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>긍정</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>긍정</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함</td>\n",
       "      <td>긍정</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>마무리는 또 왜이래</td>\n",
       "      <td>부정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      텍스트  감정  라벨\n",
       "0                                     아 더빙.. 진짜 짜증나네요 목소리  부정   0\n",
       "1                       흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나  긍정   1\n",
       "2                                       너무재밓었다그래서보는것을추천한다  부정   0\n",
       "3                           교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정  부정   0\n",
       "4       사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...  긍정   1\n",
       "...                                                   ...  ..  ..\n",
       "199995          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함  긍정   1\n",
       "199996       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO  부정   0\n",
       "199997                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다  부정   0\n",
       "199998     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네  부정   0\n",
       "199999                                         마무리는 또 왜이래  부정   0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "texts = nsmc.get_all_texts()\n",
    "labels = nsmc.get_all_labels()\n",
    "d= {\"텍스트\":texts,\"감정\":[\"긍정\" if l==1 else \"부정\" for l in labels], \"라벨\":labels}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f9f2647ac64865b04782e54d0d4ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent_analyzer.encode(texts[:1000], labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b9b2fa5c6649548e2313d6ab98fd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 74/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 75/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 76/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 77/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 78/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 79/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 80/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 81/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 82/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 83/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 84/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 85/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 86/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 87/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 88/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 89/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 90/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 91/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 92/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 93/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 94/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 95/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 96/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 97/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 98/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 99/113                        mean training loss: 0.00\n",
      "epoch: 8/10 | step: 100/113                       mean training loss: 0.00\n",
      "epoch: 8/10 | step: 101/113                       mean training loss: 0.00\n",
      "epoch: 8/10 | step: 102/113                       mean training loss: 0.00\n",
      "epoch: 8/10 | step: 103/113                       mean training loss: 0.00\n",
      "epoch: 8/10 | step: 104/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 105/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 106/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 107/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 108/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 109/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 110/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 111/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 112/113                       mean training loss: 0.01\n",
      "epoch: 8/10 | step: 113/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 1/113                         mean training loss: 0.01\n",
      "epoch: 9/10 | step: 2/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 3/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 4/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 5/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 6/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 7/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 8/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 9/113                         mean training loss: 0.00\n",
      "epoch: 9/10 | step: 10/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 11/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 12/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 13/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 14/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 15/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 16/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 17/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 18/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 19/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 20/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 21/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 22/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 23/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 24/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 25/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 26/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 27/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 28/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 29/113                        mean training loss: 0.00\n",
      "epoch: 9/10 | step: 30/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 31/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 32/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 33/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 34/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 35/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 36/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 37/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 38/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 39/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 40/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 41/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 42/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 43/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 44/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 45/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 46/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 47/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 48/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 49/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 50/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 51/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 52/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 53/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 54/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 55/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 56/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 57/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 58/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 59/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 60/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 61/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 62/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 63/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 64/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 65/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 66/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 67/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 68/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 69/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 70/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 71/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 72/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 73/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 74/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 75/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 76/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 77/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 78/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 79/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 80/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 81/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 82/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 83/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 84/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 85/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 86/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 87/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 88/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 89/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 90/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 91/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 92/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 93/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 94/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 95/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 96/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 97/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 98/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 99/113                        mean training loss: 0.01\n",
      "epoch: 9/10 | step: 100/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 101/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 102/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 103/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 104/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 105/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 106/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 107/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 108/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 109/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 110/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 111/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 112/113                       mean training loss: 0.01\n",
      "epoch: 9/10 | step: 113/113                       mean training loss: 0.01\n",
      "epoch: 10/10 | step: 1/113                        mean training loss: 0.01\n",
      "epoch: 10/10 | step: 2/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 3/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 4/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 5/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 6/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 7/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 8/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 9/113                        mean training loss: 0.00\n",
      "epoch: 10/10 | step: 10/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 11/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 12/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 13/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 14/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 15/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 16/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 17/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 18/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 19/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 20/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 21/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 22/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 23/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 24/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 25/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 26/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 27/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 28/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 29/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 30/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 31/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 32/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 33/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 34/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 35/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 36/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 37/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 38/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 39/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 40/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 41/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 42/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 43/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 44/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 45/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 46/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 47/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 48/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 49/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 50/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 51/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 52/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 53/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 54/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 55/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 56/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 57/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 58/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 59/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 60/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 61/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 62/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 63/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 64/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 65/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 66/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 67/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 68/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 69/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 70/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 71/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 72/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 73/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 74/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 75/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 76/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 77/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 78/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 79/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 80/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 81/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 82/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 83/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 84/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 85/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 86/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 87/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 88/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 89/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 90/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 91/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 92/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 93/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 94/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 95/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 96/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 97/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 98/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 99/113                       mean training loss: 0.00\n",
      "epoch: 10/10 | step: 100/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 101/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 102/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 103/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 104/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 105/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 106/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 107/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 108/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 109/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 110/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 111/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 112/113                      mean training loss: 0.00\n",
      "epoch: 10/10 | step: 113/113                      mean training loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "sent_analyzer.prepare()\n",
    "sent_analyzer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 1.52\n",
      "  Validation Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "sent_analyzer.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c65c7fe235e475e9185742fe18a33d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'긍정'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_analyzer.predict(\"재밌다고 할 수 있네\")"
   ]
  },
  {
   "source": [
    "## BERT Downstream Tasks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "====================================\n",
      "[BERT] Google Multilingual BERT 로드 완료\n",
      "====================================\n",
      "Some weights of the model checkpoint at snunlp/KR-Medium were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "====================================\n",
      "[BERT] KR-BERT 로드 완료\n",
      "====================================\n",
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "====================================\n",
      "[BERT] BERT-kor-base 로드 완료\n",
      "====================================\n",
      "Some weights of the model checkpoint at kykim/albert-kor-base were not used when initializing AlbertForMaskedLM: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "====================================\n",
      "[BERT] ALBERT-kor-base 로드 완료\n",
      "====================================\n",
      "Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "====================================\n",
      "[BERT] XLM-Roberta-kor 로드 완료\n",
      "====================================\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "====================================\n",
      "[Seq2seq + BERT] bertshared-kor-base 로드 완료\n",
      "====================================\n",
      "====================================\n",
      "[GPT3] gpt3-small-based-on-gpt2 로드 완료\n",
      "====================================\n",
      "====================================\n",
      "[ELECTRA] electra-kor-base 로드 완료\n",
      "====================================\n",
      "====================================\n",
      "[ELECTRA] koelectra-base-v3-finetuned-korquad 로드 완료\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "from winterschool_models import Winterschool_Models\n",
    "models = Winterschool_Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['M-BERT', 'KR-Medium', 'bert-kor-base', 'albert-kor-base', 'xlm-roberta-base', 'bertshared-kor-base', 'gpt3-kor-small_based_on_gpt2', 'electra-kor-base', 'electra-kor-QA'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "models.lists.keys()"
   ]
  },
  {
   "source": [
    "## Summarization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1592년 8월 14일 통영 한산 한산도 앞바다에서 조선 수군이 왜군을 크게 무찌른 해전 학익진을 처음으로 해전에서 펼쳤다. 왜왜군은 이 전투에서 육전에서 사용하던 포위 섬멸 전술 형태인 학익을 처음으로\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "한산도 대첩(閑山島大捷) 혹은 견내량대첩(見乃梁大捷)은 1592년 8월 14일(선조 25년 음력 7월 8일) 통영 한산도 앞바다에서 조선 수군이 왜군을 크게 무찌른 해전으로, 이 전투에서 육전에서 사용하던 포위 섬멸 전술 형태인 학익진을 처음으로 해전에서 펼쳤다. \n",
    "\n",
    "조선 선조 25년(1592년) 5월 29일(양력 7월 8일)에 2차 출동한 조선 수군의 전라좌수사 이순신의 함대는 6월 10일(양력 7월 18일)까지 사천, 당포, 당항포, 율포 등에서 일방적인 승리를 거두었으나, 육지에서는 패전을 거듭하고 있었다. 일본 수군은 일본 육군에 호응하여 가덕도와 거제도 부근에서 10여 척에서 30여 척까지 함대를 이루어 서진하고 있었다.\n",
    "\"\"\"\n",
    "\n",
    "models.summarize(context)"
   ]
  },
  {
   "source": [
    "## Mask prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kykim/bert-kor-base: 없어요 / 없다 / 없어\nkykim/albert-kor-base: 없다 / 없어요 / 없어\nbert_multilingual: 있다 / 한다 / 요\nkr-medium: ᄋ / そ / 5\nxlm: 없다 / 없습니다 / 없었다\n"
     ]
    }
   ],
   "source": [
    "mask_text=\"\"\"\n",
    "너 때문에 되는 일이 하나도 <mask>\n",
    "\"\"\"\n",
    "models.mask_predict(mask_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"학교는\"\n",
    "models.generate_text(word, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"korean_parallel_koen_news\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qa = \"\"\"\n",
    "애플 주식회사(영어: Apple Inc.)는 미국 캘리포니아의 아이폰, 아이맥, 맥북, 애플 TV, 맥 OS, iOS 등을 제조하는 기업이다.\n",
    "\n",
    "애플의 이전 명칭은 애플 컴퓨터 주식회사(영어: Apple Computer, Inc.)였는데, 스티브 잡스가 아이팟, 아이폰을 발표하면서 간단하게 애플이라고 기업 이름을 바꾸었다.\n",
    "\n",
    "본사는 애플 캠퍼스와 애플 파크에 두고 있으며, 미국 캘리포니아주 쿠퍼티노에 소재하고 있다. 최고경영자는 팀 쿡이다.\n",
    "\n",
    "2011년 8월 9일 미국 증시에서 장 중 엑손모빌을 누르고 시가총액 1위가 되었고, 8월 10일에는 종가에서도 1위가 되었다. 2015년 2월 11일 세계 최초로 주식 종가 시가총액이 7000억 달러를 넘은 기업이 되었다. 애플의 기업가치는 이제 2조 달러(2020년 8월 기준)가 되었다. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.8801720142364502,\n",
       " 'start': 36,\n",
       " 'end': 66,\n",
       " 'answer': '아이폰, 아이맥, 맥북, 애플 TV, 맥 OS, iOS'}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "question=\"애플은 무엇을 만드는가?\"\n",
    "models.answer_question(question, context_qa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}